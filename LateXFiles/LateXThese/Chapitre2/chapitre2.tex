\chapter{Proposition : Prise de Décision et Interruption}
	Dans ce chapitre et à l'aide de ce que nous venons d'apprendre au sujet de la répartition des tâches, nous allons pouvoir construire notre propre mécanisme générique, que nous utilisons ensuite pour notre cas d'application : la colonie d'abeilles (Chapitre 3). Nous allons voir comment modéliser nos tâches et comment les exécuter. Nous verrons ensuite les mécanismes de sélection, puis d'interruption que nous avons mis en place afin que nos agents effectuent toujours une tâche qui a du sens par rapport à l'état actuel de l'environnement, et de l'activité des autres agents. Nous terminerons ce chapitre par un exemple d'application de ce modèle dans le cadre de robotique en essaim, où une population de robot devra se partager deux tâches : collecte de ressources et patrouille.
	
	\section{Modélisation des tâches : Exécution du comportement}	
	
		\subsection{Actions, Activités et Tâches}
		\paragraph{}
			Afin de modéliser nos tâches, nous allons utiliser 3 concepts, Actions, Activités et Tâches.
			
			Une Action est définie comme une interaction avec l'environnement extérieur, non interruptible et d'une durée déterminée et courte (pas plus de quelques pas de temps, pour ne pas bloquer l'agent). Elle n'est donc pas forcément élémentaire, mais doit s'en approcher. Chaque Action possède une condition d'activation.
			
			Ensuite, une Activité est un ensemble d'Actions et/ou d'autres Activités. Une Activité possède aussi sa propre condition d'activation. Indirectement, tout ce qu'elle contient partage alors sa condition d'activation, ce qui nous permet de factoriser cette condition et d'alléger notre écriture, et ainsi de modéliser des comportements complexes.
			
			Pour finir, une Tâche est l'ensemble des Activités et Actions. On peut donc voir une Tâche comme l'Activité racine, un peu a la manière d'un système de fichier: les Activités sont des dossiers et contiennent d'autres dossiers et/ou des fichiers, que sont les Actions.
			
		\subsection{Subsomption Hiérarchique et Exécution}
		\paragraph{}
			Une architecture de subsomption permet de hiérarchiser différents comportements entre eux, afin d'obtenir un comportement général cohérent\cite{brooks_robust_1986}. Dans un ordre défini, la subsomption interroge tour à tour la condition d'activation de chacun de ses différents blocs comportements, et exécute le premier dont la condition est valide. Par exemple, modéliser le comportement d'un mouton peut se faire en deux blocs. Un premier bloc "Chercher à manger", toujours valide. Au dessus de celui ci, donc avec une priorité plus importante, un autre bloc "Brouter", s'activant lorsque le mouton a trouvé de quoi manger. Ensuite, encore au dessus, un bloc "Fuir" qui s'active dès que le mouton à perçu un prédateur dans un temps déterminé. Ainsi, tant qu'aucun grand méchant loup n'est en vue, le mouton va brouter paisiblement. Dès qu'il en verra un, alors il pourra fuir.
			
			Afin de respecter l'aspect quasi-élémentaire des comportements, le bloc "Fuir" sera réalisé une multitude de fois. Ainsi, une seule exécution de Fuir ne fera faire au mouton qu'un pas dans la direction opposée, il va y faire appel plusieurs fois avant de considérer avoir semé le loup.
			
			Une subsomption hiérarchique ajoute à cette structure simple, le fait que chaque bloc comportement puisse être une autre architecture de subsomption\cite{heckel_representational_2010}. Cette légère modification apporte une grande modularité dans la conception de ces architectures, et permet de modéliser des comportements plus complexes sans la lourdeur des subsomption classiques.
			
			Ce que nous avons appelé "bloc comportement" des subsomptions correspond à nos actions et activités. Les blocs qui contiennent une autre subsomption sont appelés activités, et ceux qui contiennent du comportement sont des actions. Ensuite, la subsomption en elle même est alors une tâche, vous trouverez Figure \ref{ModelisationTache} une représentation graphique de subsomption hiérarchique contenant nos concepts définis plus tôt.
			
			\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{Pictures/Figures/ModelisationTache.png}
			\caption{Modélisation d'une tâche à l'aide d'une subsomption hiérarchique.}
			\label{ModelisationTache}
			\end{figure}
		\paragraph{}
			Ainsi, pour qu'un agent puisse exécuter une tâche, il interroge l'activité racine puis va récursivement interroger ses composants. Chaque activité ou action interrogée va ainsi vérifier sa condition d'activation. Une activité dont l'activation est valide va alors continuer d'interroger ses composantes. On a donc une recherche en profondeur, par ordre de priorité, qui s'arrête à la première action interrogée avec une condition d'activation valide. Cette action est alors remontée à l'agent, qui pourra alors l'exécuter pendant toute sa durée. Ensuite, une fois l'action terminée, tout ce processus recommence afin de pouvoir récupérer une nouvelle action à exécuter.
			
			\paragraph{}
			Pour reprendre l'exemple du mouton, nous pouvons complexifier son comportement, en transformant son action "Fuir", en une activité "Fuir" contenant deux actions. L'action prioritaire "Esquiver" a pour condition le fait de voir le loup droit devant, et consiste à fuir mais en tournant, afin d'éviter le loup. Ensuite, la deuxième action, "Pleine Puissance" est l'action par défaut, sans condition, qui consiste à courir tout droit tant que le loup n'a pas été vu depuis un certain temps.
			
			La condition d'activation de l'activité "Fuir" définie plus tôt, le fait d'avoir vu un prédateur dans les dernières secondes/minutes, est alors nécessaire à l'activation des deux actions "Esquiver" et "Pleine Puissance" sans que nous ayons à les réécrire explicitement.
			
	\section{Motivation : État de l'art}

Avant de continuer à construire notre modèle, nous avons besoin d'un mécanisme nous permettant d'améliorer les modèles a seuils dont nous avons discuté dans l'état de l'art. En effet, ceux-ci ne permettent pas de modéliser des tâches n'ayant pas de stimulus déclencheur : par exemple, la faim est un stimulus déclencheur de l'action de manger. En revanche, le stimulus déclencheur de rédiger un état de l'art, ou de ranger sa chambre, est beaucoup plus complexe. Nous proposons donc une solution simple, permettant de prendre en compte ces tâches sans stimulus tout en utilisant un modèle à seuils : en utilisant la motivation interne de l'agent. Avant de plonger dans notre utilisation de la motivation, voici un rapide état de l'art sur son utilité, et notre positionnement par rapport à celui-ci.

\paragraph{}
Pour les psychologues, la motivation est la source de l'action, et guide son exécution. Deux types de motivations existent: extrinsèque (ou externe), lorsqu'une récompense est offerte par le monde extérieur, et intrinsèque (ou interne), qui n'a à voir qu'avec les croyances ou besoins de l'agent, comme l'amusement ou la curiosité. La théorie du Flow\cite{csikszentmihalyi_finding_1997}, de son côté, dit que la motivation interne d'un individu est maximale lorsque la difficulté rencontrée lors de la réalisation d'une tâche est suffisante pour susciter l'intérêt (qu'elle n'est pas ennuyeuse) mais suffisamment faible pour ne pas être décourageante (qu'elle n'est pas impossible à réaliser pour l'agent). 

On note dès lors que la motivation interne présente dans la littérature peut se diviser en deux catégories : d'un côté la motivation source, comme la faim, qui va provoquer un comportement, et de l'autre côté la motivation guide, motivation au sens d'implication, d'intérêt, qui elle sera plutôt un guide de cette action, comme la curiosité. On retrouve ces notions en éthologie, par exemple chez Lorenz\cite{lorenz_les_1984}, pour qui la motivation interne (guide), couplée à un stimulus (source), va déclencher et entretenir un comportement.
        
        
        \paragraph{}
        En intelligence artificielle, la motivation intrinsèque \textit{source} est particulièrement utilisée pour les systèmes d'apprentissage \cite{schmidhuber_formal_2010}, \textit{e.g.} pour aider ou guider des agents apprenants \cite{baldassarre_intrinsically_2013}, notamment en apportant la notion de curiosité à des agents informatiques. Certains travaux  \cite{carbonell_multi-agent_1994, maes_agent_1991} s'attachent à sa définition proche de l'éthologie, dans laquelle la motivation intrinsèque peut venir de nombreux stimulus internes différents et plus primitifs, tels que la faim ou la peur.
        
        D'autres travaux se basent sur la théorie du Flow : Un agent qui ne parvient pas à réaliser sa tâche ressent de l'anxiété et cherche une tâche moins difficile\cite{cornudella_how_2015}. De la même manière, un agent qui accomplit une tâche facile s'ennuie et passe à des tâches plus difficiles. L'idée de compétence apportée par Roohi et al.\cite{roohi_review_2018} rejoint ces notions : la compétence est, pour un agent, le sentiment d'être en contrôle et capable d'accomplir sa tâche actuelle. Ainsi, un agent ayant un niveau de compétence qu'il juge trop faible pour la tâche actuelle cherchera une tâche plus facile, plus adaptée.
        
        \paragraph{}
        Nous distinguons donc bien deux catégories dans la motivation interne. La première, que nous allons continuer à appeler la motivation source, proche de l'éthologie, décrit une motivation comme un stimulus interne servant a déclencher des comportements. Pour reprendre l'exemple de notre mouton, si voir un loup ne déclenche pas de réaction physique directe (au final ce ne sont que des pixels plus sombres sur le fond de sa rétine), son cerveau va pourtant reconnaitre le loup et faire augmenter la soudaine motivation de prendre ses jambes à son coup. On peut le voir comme c'était la peur qui déclenchait la fuite.
        
        La deuxième, la motivation guide, proche de l'idée du \textit{Flow}, permet à l'agent de se situer par rapport à la difficulté de la tâche qu'il exécute, afin de maximiser son apprentissage, et donc d'optimiser l'usage de son temps. L'exemple de notre cher mouton sera ici un peu tiré par les cheveux, mais nous nous y tiendrons : nous souhaitons apprendre à ce mouton comment résoudre un rubix cube. Si le casse tête lui est donné sans introduction, la tâche est insurmontable, décourageante. Le mouton n'apprend rien, il perd son temps et va donc naturellement se déconcentrer et essayer autre chose (peut être essayera-t-il d'apprendre à jongler avec plusieurs cubes ?). En revanche, si la courbe de difficulté est adaptée, en exercices simples et incrémentaux, le mouton apprendra bien mieux et restera concentré. La difficulté est toujours adaptée à ses compétences.
        
        \paragraph{}
        Avec ces deux notions en tête, nous pouvons passer à la suite de la description de notre modèle.
			
	\section{Sélection : Modèle à Seuil}
		Maintenant que nous avons modélisé le fonctionnement interne de nos tâches, nous allons pouvoir construire le mécanisme permettant à nos agents de sélectionner la plus prioritaire.
		
			Pour cette sélection nous allons utiliser un modèle à seuil, que nous allons légèrement adapter en lui ajoutant un mécanisme d'interruption, décrit dans la section \ref{sectionInterruption}. Dans un modèle a seuil, chaque tâche possède une fonction lui permettant de calculer son score. Un agent peut ainsi comparer le score de toutes ses tâches, et sélectionner la tâche qui possède le plus élevé. Très souvent lié à un stimulus déclencheur, le score de la tâche est calculé à l'aide d'une fonction sigmoïde, paramétrée par un seuil, qui prend en entrée le stimulus (ou une combinaison linéaire de plusieurs stimulus), et nous donne en résultat le score de la tâche, comme nous avons pu le constater dans l'état de l'art.	
			
			Même si plusieurs agents sont en mesure d'effectuer la même tâche, les seuils de ses tâches lui sont propres. Chaque agent possède une instance différente des tâches, et chaque instance de tâche possède son propre seuil. En modélisant nos tâches, il arrive que certaines n'aient pas de stimulus déclencheur. C'est le cas pour nos abeilles butineuses : le butinage de nectar semble être leur comportement par défaut, aucun stimulus global n'a encore été identifié. Dans ce cas, nous appliquons la notion de motivation interne \textit{source}, que nous avons décrit juste avant, pour créer un stimulus artificiel, qui sera inséré dans la sigmoïde afin de calculer le score de la tâche, comme les autres.
			
			\paragraph{}
			Afin de toujours réaliser une tâche utile à la communauté, nos agents doivent très régulièrement interroger leurs perceptions, afin de se "mettre à jour". C'est pour ceci que nous avons décider de la notion d'évaluation systématique : un agent réévalue l'ensemble de ses tâches à chaque fois qu'il termine une action. C'est également pour cette raison que les actions doivent avoir une durée courte, de préférence atomique, pour permettre la mise à jour. Ce rafraichissement des perceptions est essentiel pour que le système puisse réagir face à une urgence. Même si notre mouton est paisiblement en train de brouter, il est intuitif de l'autoriser à fuir à la vue d'un loup avant d'avoir parfaitement terminé de brouter. Il doit aussi pouvoir faire une pause lors de la résolution d'un casse tête afin de se nourrir, pour ne pas mourir de faim.
		
	\section{Interruption : Motivation et Flow}
	\label{sectionInterruption}
		\subsection{Action Démotivante et Tâche Motivée}
		
			La réévaluation systématique nous autorise à ne pas avoir de mécanisme d'interruption : dans le cas général, la fluctuation des stimulus internes et externes suffit à l'agent pour effectuer la bonne tâche. En revanche, la question se pose lorsqu'une tâche possède un stimulus déclencheur artificiel, lié à une motivation source, fixe. Pour décider quand arrêter une telle tâche, nous avons besoin de ce mécanisme d'interruption. Dans ce cas, nos agents vont essayer d'estimer leur apport à la société dans leur tâche actuelle. Ainsi, lorsqu'un agent verra qu'il n'arrive pas a réaliser sa tâche, il sera dans l'état de malaise décrit pas le \textit{Flow} et cherchera de plus en plus à changer de tâche. Nous cherchons alors à mesurer l'efficacité de chaque agent dans sa tâche, afin de pouvoir détecter lorsqu'il arrive a la réaliser, mais surtout lorsqu'il n'y arrive pas. Il suffit alors de déterminer dans le comportement de l'agent, quelles sont les Actions effectuées représentative de l'échec de la tâche. Par exemple, un robot ayant pour tâche de récolter des ressources aura dans cet algorithme une séquence de déplacement aléatoire lorsqu'aucune ressource n'est en vue. Répéter en boucle ce déplacement aléatoire, cette Action, est un signe que ce robot n'arrive pas a correctement réaliser sa tâche, il devrait donc essayer de faire autre chose.
			
			Nous proposons alors ajouter aux Actions définies plus tôt le fait de pouvoir être "Démotivante". Lors de l'exécution d'une Action "Démotivante", un agent va baisser sa motivation interne d'un montant défini. Le but est d'augmenter les chances qu'il abandonne cette tâche au profit d'une autre, lors du processus de sélection que nous allons détailler plus tard. L'Action de déplacement aléatoire du robot que nous venons de citer est un bon exemple d'Action démotivante. Nous pouvons alors augmenter notre concept de tâche, en disant qu'une Tâche est "motivée" lorsqu'elle contient au moins une action démotivante.
			
			\paragraph{}
			Valeur de motivation ? bénéfices attendus, lien avec les seuils ...
	
	\section{Définir un Agent}
	
	\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Pictures/Figures/ModelisationExecution.png}
	\caption[Sélection et exécution des tâches par chaque Agent, à chaque pas de temps.]{Sélection et exécution des tâches par chaque Agent, à chaque pas de temps. Si l'action en cours est terminée, l'agent va sélectionner une nouvelle tâche, en extraire l'action à réaliser puis l'exécuter.}
	\label{agentExec}
	\end{figure}		
	
		\paragraph{}
		À l'aide de ces définitions nous pouvons désormais décrire nos agents. Un agent est situé dans l'environnement, possède une série de senseurs internes et externes (comme la faim et l'odorat), et contient aussi une liste de tâches qu'il sera peut-être amené à réaliser au court de sa vie. Lors d'une sélection de tâche, l'agent pourra confronter ses perceptions courantes aux différents seuils et conditions de l'algorithme de sélection de tâche, afin de savoir quelles actions effectuer.
		
		Parmi les perceptions internes de l'agent, nous trouvons une variable de motivation interne, servant aussi à la sélection de tâche. Associer une valeur de motivation à chaque tâche aurait aussi été possible et nous pouvons en trouver des équivalence dans d'autres travaux, nous avons donc décider de tenter l'expérience avec une valeur transversale à toute les taches, liée à l'agent lui même.
		
		\paragraph{}
		Un agent peut aussi présenter des variations individuelles, un léger \textit{offset} que nous pouvons utiliser pour ajuster légèrement les seuils de ses différentes tâches, en plus des conditions internes et externes, afin de créer une population d'agent plus ou moins homogène. Via ses tâches, l'agent possède des seuils variables qui lui sont propres : deux agents avec les mêmes tâches présentent le plus souvent des seuils différents.
		
		Seuils = états internes
		
	\section{Application Robotique en Essaim}
	
		\paragraph{}
			Les systèmes multi-agents sont souvents utilisés dans la mise en place d'essaims de robots, "Swarm Robotics". Ces essaims consistent en une grande quantité (dizaines, voire centaines) de robots simples, amenés à exécuter des tâches complexes en collectivité. L'image de la capacité de fourragement des fourmis est souvent utilisée comme cas d'application, nous avons donc construit notre exemple dans ce contexte. Un ensemble de robots va devoir ramener des ressources à leur base commune. Les ressources sont éparpillées dans l'environnement et doivent être traitées par un robot avant d'être déplacées jusqu'à la base. En plus de cette activité de collecte, les robots vont devoir assurer une surveillance de la base, en patrouillant autour. Ces robots possèdent une mémoire limitée : ils connaissent leur position ainsi que celle de la base, et peuvent se rappeler de la position de gisement de minerai qu'ils ont directement observé (Nous pourrions ajouter une communication courte portée, où les robots pourraient échanger des coordonnées de gisement, mais ceci sort du cadre de cet exemple ?).
			
		De plus, nous ajoutons la notion d'outil : Un robot devra posséder le bon outil pour exécuter une tâche, une pioche pour collecter des ressources, et des jumelles pour patrouiller. Les ressources brutes devront être traitée sur place avant de pouvoir être collectées puis amené à la base.

		\paragraph{}
		Nous pouvons dès lors commencer à construire nos tâches :
		\begin{itemize}
			\item \textbf{Patrouiller} : Le robot effectue des cercles larges autour de la base, observant les alentours. Il se démotive légèrement au fil du temps, et un peu plus lorsqu'il croise un autre robot. Seuil élevé, sauf si l'agent est équipé des jumelles.
			\item \textbf{Collecter} : Le robot parcours l'environnement aléatoirement à la recherche de gisement. Une fois trouvée, il traite le gisement pour collecter des ressources, puis les ramène a la base. Il se démotive légèrement à chaque pas de temps où il exécute un déplacement aléatoire. Seuil élevé, sauf si l'agent est équipé d'une pioche.
			\item \textbf{Recharger} : Le robot se connecte a la base pour recharger ses batteries.
			\item \textbf{Mémoriser} : Lorsque le robot voit un gisement qu'il ne connait pas, il l'ajoute à sa mémoire. A l'inverse, lorsqu'il ne voit pas de gisement (ou qu'il voit un gisement épuisé) là où il en avait retenu un, il l'oublie.
		\end{itemize}
	
	\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Pictures/Figures/SwarmPatrol.png}
	\caption{Robotique en essaim : Modélisation de la tâche de patrouille.}
	\label{swarmPatrol}
	\end{figure}
	
	\begin{figure}	
	\centering
	\includegraphics[width=\textwidth]{Pictures/Figures/SwarmForaging.png}
	\caption{Robotique en essaim : Modélisation de la tâche de collecte de ressources.}
	\label{swarmForaging}
	\end{figure}
		
		Pour \textit{Patrouiller} et \textit{Collecter}, si l'agent active la tâche sans posséder le bon outil, la tâche commencera par le faire retourner a la base afin d'équiper le bon outil. Équiper le bon outil va alors changer les seuils de ces deux tâches, priorisant celle dont l'outil est le bon. Ainsi, un agent ne changera d'outil que lorsque cela sera nécessaire, les seuils ajoutent ici une notion de coût du changement d'outil. Ensuite, si nous le souhaitons, en ajustant les valeurs des seuils bas (prioritaires) et haut (changement d'outil nécessaire) et/ou la force répulsive des actions démotivantes, nous pouvons ajuster ce coût empiriquement. Vous retrouverez notre modélisation en subsomption hiérarchique appliquée à la tâche de patrouille Figure \ref{swarmPatrol}, ainsi que celle de tâche de collecte de ressources Figure \ref{swarmForaging}.
		
		Ensuite, si \textit{Recharger} a un stimulus déclencheur évident, le niveau courant de batterie, ce n'est pas le cas pour les deux autres tâches. Nous leur appliquons donc une motivation source. Nous pouvons ensuite modifier ces motivations sources avec des perceptions de l'agent, sans qu'elles soient la majorité du stimulus déclencheur : Nous pouvons réduire légèrement cette stimulation pour la tâche \textit{Patrouiller} lorsqu'un robot avec des jumelles est dans le champs de vision. De même, nous pouvons réduire la stimulation de \textit{Collecter} lorsqu'un robot avec une pioche est en vue, et l'augmenter lorsqu'un gisement de minerai est en vue.
			
			
				
	\section*{Conclusion}
		\paragraph{}
		La sélection de tâche se fait via un système à seuil adapté afin de prendre en compte les motivations internes source (nous venant plutôt de l'éthologie) et guide (nous venant de la théorie du \textit{Flow}), afin d'inclure des tâches ne présentant pas de stimulus déclencheur définit. Un score est calculé par tâche, et l'agent sélectionne la tâche qui possède le plus grand score. Lorsque la tâche précédemment réalisée est une tâche motivée, le score est remplacé par la motivation guide, interne à l'agent et transversale à toutes les tâches de cet agent. Une fois la tâche sélectionnée, l'agent utilise notre architecture de tâche en subsomption hiérarchique afin d'obtenir le comportement qu'il doit réaliser. Nous avons vu ensemble un exemple d'application à un essaim de robot, et nous allons désormais pouvoir aborder l'implémentation de l'application principal de ces travaux, la colonie d'abeilles.
