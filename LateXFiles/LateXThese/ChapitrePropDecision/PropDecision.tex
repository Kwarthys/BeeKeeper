\chapter{Proposition : Prise de Décision et Interruption}
\label{ChapitrePropDecision}
	Dans ce chapitre et à l'aide de ce que nous venons d'apprendre au sujet de la répartition des tâches, nous décrivons notre propre mécanisme générique, que nous utilisons pour notre cas d'application : la colonie d'abeilles (Chapitre \ref{ChapitrePropSMA}). Nous allons voir comment modéliser nos tâches et comment les faire exécuter par nos agents. Nous verrons ensuite les mécanismes de sélection puis d'interruption que nous avons mis en place afin que nos agents effectuent toujours une tâche qui a du sens par rapport à l'état actuel de l'environnement et à l'activité des autres agents. Pour ceci nous mettons en place un modèle à seuils, et ajoutons le concept de motivation interne à nos agents afin qu'ils puissent interrompre leur tâche en cours s'ils le jugent nécessaire. Cette méthode nous permet d'intégrer au modèle à seuils des tâches qui ne possèdent pas de stimulus déclencheurs dans l'environnement, ce dont les modèles à seuils sont classiquement incapables. Nous terminerons ce chapitre par un exemple possible d'application de ce modèle dans le cadre de la robotique en essaim, où une population de robots devra se partager deux tâches : collecte de ressources et patrouille.
	
	\section{Modélisation des Tâches et du Comportement des Agents}	
	
		\subsection{Actions, Activités et Tâches}
		\paragraph{}
			Afin de modéliser nos tâches, nous utilisons 3 concepts : Actions, Activités et Tâches. Plusieurs définitions différentes existent dans la littérature. Un point qui revient souvent est que le "rôle" est un concept abstrait \cite{ferber_meta-model_1998, wooldridge_methodology_1999, campbell_multi-agent_2011}, raison pour laquelle il n'apparaît pas à ce niveau de notre modèle, malgré son omniprésence dans le domaine Multi-Agents. Ici et dans la littérature, un rôle peut être utilisé pour qualifier des agents qui réalisent certaines tâches particulières. Un rôle peut ainsi englober plusieurs tâches, une seule, ou même ne concerner qu'une sous-partie d'une tâche. Le rôle n'a pas d'intérêt pour le système, en revanche, il est utile à l'observateur pour mieux analyser, communiquer et décrire ce qu'il voit. Tâches et Activités sont souvent utilisées de manière très différentes, et parfois interchangeable, mais nous nous rapprochons de la vision de Krieger et al. \cite{krieger_call_2000}, disant qu'une Tâche correspond à ce qui doit être fait, là où une Activité définit plutôt ce qui est en train d'être fait. Ainsi nos Tâches représentent un travail à réaliser dans sa globalité, là où l'Activité est plus proche de ce que va réaliser un agent, plus concret. Nous ajoutons à ceci le concept d'Action, qui se glisse dans cette définition décrivant le travail élémentaire réalisé par un agent à un instant t. Le lecteur peut trouver dans ce qui suit les définitions précises de ce que nous entendons dans ce manuscrit par ces trois termes, Action, Activité et Tâches.
			
			Une \textbf{Action} est définie comme une interaction avec l'environnement extérieur, non interruptible et d'une durée déterminée et courte (pas plus de quelques pas de temps de simulation, pour ne pas bloquer l'agent). Elle n'est donc pas forcément élémentaire, mais doit s'en approcher. Chaque Action possède une condition d'activation.
			
			Ensuite, une \textbf{Activité} est un ensemble d'Actions et/ou d'autres Activités. Une Activité possède aussi sa propre condition d'activation. Indirectement, tout ce qu'elle contient partage alors sa condition d'activation, ce qui nous permet de factoriser cette condition et d'alléger notre écriture, et ainsi de modéliser des comportements élaborés.
			
			Pour finir, une \textbf{Tâche} contient l'ensemble des Activités et Actions concernant un même comportement général. On peut donc voir une Tâche comme l'Activité racine, un peu à la manière d'un système de fichiers : les Activités sont des dossiers et contiennent d'autres dossiers et/ou des fichiers, que sont les Actions. Une Tâche est alors le dossier racine, le \textit{root}, de cet ensemble de fichier. La Tâche assure aussi le lien entre les Activités et Actions et le modèle de sélection de tâche définit dans la section suivante, à base de stimulus déclencheur et de système à seuils.
			
			Notre concept d'Action peut se rapprocher du concept de \textit{primitive} avancé par Drogoul et al. \cite{drogoul_multi-agent_1992}, et nos Tâches et Activités sont proches de leur concept de \textit{tâche}. En effet, leurs \textit{primitives} sont des comportements bas niveau élémentaires, que l'agent n'exécute qu'à travers l'exécution de \textit{tâches}, qui regroupent un ensemble de \textit{primitives}. De plus, leurs \textit{tâches} permettent aussi de faire le lien entre \textit{primitives} et stimulus déclencheur. Ils ne parlaient pas encore de modèles à seuils, mais leur modèle de sélection en est relativement proche (décrit Section \ref{sectionAppli}).
			
		\subsection{Subsomption Hiérarchique et Exécution}
		
			Nous organisons et formalisons nos Tâches en utilisant des architectures de subsomptions hiérarchiques. Celles-ci permettent de classer différents comportements entre eux afin d'obtenir un comportement général cohérent \cite{brooks_robust_1986}. Dans un ordre défini, la subsomption interroge tour à tour la condition d'activation de chacun de ses différents blocs comportements, et exécute le premier dont la condition est valide. Par exemple, modéliser le comportement d'un mouton peut se faire en trois blocs. Un premier bloc "Chercher à manger", toujours valide. Au-dessus de celui-ci, donc avec une priorité plus importante, nous plaçons un autre bloc : "Brouter". Ce dernier s'active lorsque le mouton a trouvé de quoi manger. Enfin, nous plaçons au sommet le bloc "Fuir", s'activant dès que le mouton perçoit un prédateur, et reste activé le temps de le semer. Ainsi, tant qu'aucun loup n'est en vue, le mouton va brouter paisiblement. Dès qu'il en verra un, alors il pourra fuir.
			
			Afin de respecter l'aspect quasi-élémentaire des comportements, le bloc "Fuir" sera réalisé une multitude de fois. Ainsi, une seule exécution de Fuir ne fera faire au mouton qu'un pas l'éloignant du prédateur. Il va y faire appel plusieurs fois avant de considérer avoir semé le loup, tant que la condition du bloc "Fuir" sera valide.
			
			Une subsomption hiérarchique ajoute à cette structure simple, le fait que chaque bloc comportement puisse être une autre subsomption \cite{heckel_representational_2010}. Cette légère modification apporte une grande modularité dans la conception de ces architectures, et permet de modéliser des comportements plus élaborés sans la lourdeur des subsomptions classiques.
			
			\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{Pictures/Figures/ModelisationTache.png}
			\caption{Modélisation d'une Tâche à l'aide d'une subsomption hiérarchique.}
			\label{ModelisationTache}
			\end{figure}
		\paragraph{}
			
			Ce que nous avons appelé "bloc comportement" des subsomptions correspond à nos Actions et Activités. Les blocs qui contiennent une autre subsomption sont appelés Activités, et ceux qui contiennent du comportement sont des Actions. Ensuite, la subsomption en elle-même est alors une Tâche. La Figure \ref{ModelisationTache} présente en une Tâche abstraite la notion de subsomption hiérarchique contenant nos concepts définis plus tôt. Les Activités y sont représentées en rectangles arrondis et les Actions ont les traits les plus fins. Une bulle permet de représenter la subsomption présente à l'intérieur de l'Activité 1. Les conditions de chaque bloc sont représentées par des losanges placés à côté de leur bloc correspondant. Un bloc ne sera valide que lorsque sa condition le sera aussi.
			
			Ainsi, pour qu'un agent puisse exécuter une Tâche, il interroge l'Activité racine puis va récursivement interroger ses composants. Chaque Activité ou Action interrogée va ainsi vérifier sa condition d'activation. Une Activité dont l'activation est valide va alors continuer d'interroger ses composantes. On a donc une recherche en profondeur, par ordre de priorité, qui s'arrête à la première Action interrogée avec une condition d'activation valide. Cette Action est alors remontée à l'agent, qui pourra l'exécuter pendant toute sa durée. Ensuite, une fois l'Action terminée, tout ce processus recommence afin de pouvoir déterminer une nouvelle Action à exécuter.
			
			\begin{figure}
			\centering
			\includegraphics[width=.9\textwidth]{Pictures/Figures/ExMouton.png}
			\caption{Modélisation de la Tâche "Se nourrir en extérieur" contenant tout le comportement du mouton de notre exemple.}
			\label{TacheMouton}
			\end{figure}
			
			Pour reprendre l'exemple du mouton, nous pouvons complexifier son comportement en transformant son Action "Fuir" en une Activité "Fuir" contenant deux Actions. L'Action prioritaire "Esquiver" a pour condition le fait de voir le loup droit devant, et consiste à fuir mais en tournant, afin d'éviter le loup. Ensuite, la deuxième Action, "Pleine Puissance" est l'Action par défaut, sans condition, qui consiste à courir tout droit tant que ça ne fait pas suffisamment de temps que le loup n'a pas été vu. La condition d'activation de l'Activité "Fuir" définie plus tôt, qui est validée lorsqu'un prédateur a été vu dans les dernières secondes/minutes, est alors nécessaire à l'activation des deux Actions "Esquiver" et "Pleine Puissance" sans que nous ayons à les réécrire explicitement. La Figure \ref{TacheMouton} reprend la structure de la tâche du mouton que nous décrivons dans cette section. La Figure \ref{ModelisationTache} utilise la représentation classique d'une subsomption, mais pour plus de clarté nous utiliserons désormais systématiquement la représentation des subsomptions sous la forme de la Figure \ref{TacheMouton}. Nous y observons une Tâche représentée par un rectangle au trait épais, et nommée "Se nourrir en extérieur". Elle contient une Activité "Fuir", en haut donc la plus prioritaire, et représentée par un rectangle au trait moyennement épais. La Tâche contient aussi deux Actions, rectangles aux traits fins, par ordre de priorité "Brouter" et "Chercher à Manger". L'Action la moins prioritaire ne possède pas de condition, qui sont ailleurs représentées par des losanges, c'est une Action par défaut : si aucun autre bloc plus prioritaire n'a sa condition validée, cette dernière Action est toujours valide. Ensuite, l'Activité "Fuir" contient elle-même deux actions, "Esquiver" et "Pleine Puissance", avec cette dernière en Action par défaut ne contenant aucune condition et "Esquiver", prioritaire par rapport à "Pleine Puissance". Nous observons donc dans cette figure l'architecture en subsomption hiérarchique, grâce à l'Activité "Fuir" qui contient une nouvelle subsomption.
			
	
			
	\section{Sélection de Tâche : Modèle à Seuils}
		Maintenant que nous avons modélisé l'exécution du comportement d'une Tâche par un agent, nous allons pouvoir construire le mécanisme permettant à nos agents de sélectionner la Tâche la plus prioritaire, parmi toutes celles qu'il est capable de réaliser.
		
		\subsection{Sélection avec un RTM Classique}
			Pour cette sélection nous allons utiliser un modèle à seuils, que nous allons légèrement adapter en lui ajoutant un mécanisme d'interruption décrit dans la Section \ref{sectionInterruption}. Dans un modèle à seuils, chaque Tâche possède une fonction lui permettant de calculer son score. Un agent peut ainsi sélectionner la Tâche qui possède le plus élevé. Lié à un stimulus déclencheur, le score de la Tâche est calculé à l'aide d'une fonction sigmoïde, paramétrée par un seuil, qui prend en entrée le stimulus (ou une combinaison linéaire de plusieurs stimulus), et nous donne en résultat le score de la Tâche, comme nous avons pu le constater dans l'état de l'art, Section \ref{subsectionRTM}. 
			
			Dans notre modèle, nous modifions la valeur des seuils en fonction de l'état interne (physiologique et/ou physique) de nos agents. Même si plusieurs agents sont en mesure d'effectuer la même Tâche, les seuils de ses Tâches leurs sont propres. Chaque agent possède une instance différente des Tâches, et chaque instance de Tâche possède son propre seuil. 
			
			\paragraph{}
			Afin de toujours réaliser une tâche utile à la communauté, nos agents doivent très régulièrement interroger leurs perceptions, afin de prendre des décisions informées. C'est pour ceci que nous avons introduit la notion d'évaluation systématique : un agent réévalue l'ensemble de ses Tâches à chaque fois qu'il termine une Action. C'est également pour cette raison que les Actions doivent avoir une durée courte, de préférence atomique, pour permettre la mise à jour. Ce rafraichissement des perceptions est essentiel pour que le système puisse réagir face à une urgence. Même si notre mouton est paisiblement en train de brouter, il est intuitif de l'autoriser à fuir à la vue d'un loup avant d'avoir parfaitement terminé de brouter. Il doit aussi pouvoir faire une pause lors de la résolution d'un casse-tête (nous en reparlerons) afin de se nourrir, pour ne pas mourir de faim.
		
	%\section{Interruption : Motivation et Flow}
	%\label{sectionInterruption}

En modélisant nos Tâches, il arrive que certaines n'aient pas de stimulus déclencheur. C'est le cas pour les abeilles butineuses : le butinage de nectar semble être leur comportement par défaut, aucun stimulus global n'a encore été identifié. Pour continuer à construire notre modèle, nous avons besoin d'un mécanisme nous permettant d'améliorer les modèles à seuils dont nous avons discuté dans l'état de l'art. En effet, ceux-ci ne permettent pas de modéliser des tâches n'ayant pas de stimulus déclencheur : par exemple, la faim est un stimulus déclencheur de l'action de manger. En revanche, le stimulus déclencheur de "résoudre un casse-tête", "ranger sa chambre" ou encore "rédiger un état de l'art", est beaucoup plus complexe. Nous proposons donc une solution simple, permettant de prendre en compte ces tâches sans stimulus tout en utilisant un modèle à seuils : en utilisant la motivation interne de l'agent pour la sélection et l'interruption des Tâches. Avant d'aller plus loin dans la description de notre utilisation de la motivation, voici un rapide état de l'art sur son usage dans la littérature, et notre positionnement par rapport à celui-ci.
	
	\subsection{Motivation : Point sur la Littérature}

\paragraph{}
Pour les psychologues, la motivation est la source de l'action et guide son exécution. Deux types de motivations existent : extrinsèque (ou externe), lorsqu'une récompense est offerte par le monde extérieur, et intrinsèque (ou interne), qui n'a à voir qu'avec les croyances ou besoins de l'agent, comme l'amusement ou la curiosité. La théorie du Flow \cite{csikszentmihalyi_finding_1997}, de son côté, dit que la motivation interne d'un individu est maximale lorsque la difficulté rencontrée lors de la réalisation d'une tâche est suffisante pour susciter l'intérêt (qu'elle n'est pas ennuyeuse) mais suffisamment faible pour ne pas être décourageante (qu'elle n'est pas impossible à réaliser pour l'agent). 

On note dès lors que la motivation interne présente dans la littérature peut se diviser en deux catégories : d'un côté la motivation source, comme la faim, qui va provoquer un comportement, et de l'autre côté la motivation guide, motivation au sens d'implication, d'intérêt, qui elle sera plutôt un guide de cette action, comme la curiosité. On retrouve ces notions en éthologie, par exemple chez Lorenz \cite{lorenz_les_1984}, pour qui la motivation interne (guide), couplée à un stimulus (source), va déclencher et entretenir un comportement.
        
        
        \paragraph{}
        En intelligence artificielle, la motivation intrinsèque \textit{source} est particulièrement utilisée pour les systèmes d'apprentissage \cite{schmidhuber_formal_2010}, \textit{e.g.} pour aider ou guider des agents apprenants \cite{baldassarre_intrinsically_2013}, notamment en apportant la notion de curiosité à des agents informatiques, ce qui peut se rapprocher d'une motivation guide. Certains travaux  \cite{drogoul_multi-agent_1992, maes_agent_1991} s'attachent à sa définition proche de l'éthologie, dans laquelle la motivation intrinsèque peut venir de nombreux stimulus internes différents et plus primitifs, tels que la faim ou la peur.
        
        D'autres travaux se basent sur la théorie du Flow : un agent qui ne parvient pas à réaliser sa tâche ressent de l'anxiété et cherche une tâche moins difficile \cite{cornudella_how_2015}. De la même manière, un agent qui accomplit une tâche facile s'ennuie et passe à des tâches plus difficiles. L'idée de compétence apportée par Roohi et al. \cite{roohi_review_2018} rejoint ces notions : la compétence est, pour un agent, le sentiment d'être en contrôle et capable d'accomplir sa tâche actuelle. Ainsi, un agent ayant un niveau de compétence qu'il juge trop faible pour la tâche actuelle cherchera une tâche plus facile, plus adaptée.
        
        \paragraph{}
        Nous distinguons donc bien deux catégories dans la motivation interne. La première, que nous allons continuer à appeler la Motivation Source, proche de l'éthologie, décrit une motivation comme un stimulus interne servant à déclencher des comportements. Pour reprendre l'exemple de notre mouton, nous pouvons dire que c'est la faim, stimulus interne, qui le "motive" à brouter. De plus, si voir un loup ne déclenche pas de réaction physique directe (au final ce ne sont que des pixels plus sombres sur le fond de sa rétine), son cerveau va pourtant reconnaitre le loup et faire augmenter la soudaine motivation de prendre ses jambes à son cou. On peut le voir comme si c'était la peur, stimulus interne, qui déclenchait la fuite, en réponse à la réception d'un stimulus externe, la vision du loup. La peur est donc une Motivation Source.
        
        La deuxième, la Motivation Guide, proche de l'idée du \textit{Flow}, permet à l'agent de se situer par rapport à la difficulté de la tâche qu'il exécute, afin de maximiser son apprentissage, et donc d'optimiser l'usage de son temps. L'exemple de notre mouton sera ici quelque peu improbable, mais nous nous y tiendrons : nous souhaitons apprendre à ce mouton à résoudre un \textit{Rubik's Cube}. Si le célèbre casse-tête lui est donné sans introduction, la tâche est insurmontable, décourageante. Le mouton n'apprend rien, il perd son temps et va donc naturellement se déconcentrer et essayer autre chose \footnote{peut être essayera-t-il d'apprendre à jongler avec plusieurs cubes ?}, il n'est pas dans un état de \textit{Flow}. En revanche, si la courbe de difficulté est adaptée, en exercices simples et incrémentaux, le mouton apprendra bien mieux et restera concentré : la difficulté sera alors toujours adaptée à ses compétences, il est dans un état de \textit{Flow}.
        
        Nous trouvons notamment Agassounon et al. \cite{agassounon_scalable_2001} qui utilisent une mesure locale de l'efficacité de robots pour leur permettre de choisir entre deux tâches. Les transitions entre ces deux tâches étant explicites, nous souhaitons étendre ce raisonnement afin que nos agents puissent prendre des décisions sur un nombre indéfini de tâches. Pour ceci nous utilisons les transitions implicites offertes par les modèles à seuils couplées à la mesure locale de l'efficacité, que nous concentrons dans la motivation guide.
        
        \paragraph{}
        Avec ces deux notions en tête, Motivation Source et Guide, nous pouvons passer à la suite de la description de notre modèle, dont la sélection et l'interruption des tâches se feront dorénavant grâce à ces motivations.
        
        
		\subsection{Action Démotivante et Tâche Motivée}
		\label{sectionInterruption}
		
		Nous allons décrire dans cette section notre utilisation de la Motivation Source comme stimulus artificiel pour les Tâches sans stimulus associé, et la Motivation Guide comme mécanisme d'interruption de ces mêmes Tâches, alors dites "Tâches Motivées".
		
		\subsubsection{Stimulus Artificiel}
			Dans le cas de Tâches dont aucun stimulus déclencheur ne peut être trouvé, nous appliquons la Motivation Source et proposons de créer un stimulus déclencheur artificiel, d'une valeur donnée, qui peut être fixe mais que nous pourrions faire varier au besoin. Ce stimulus artificiel viendra agir exactement comme si l'agent percevait ce stimulus, et sera alors traité par la fonction de calcul de score de la Tâche comme un stimulus classique. Ainsi, toutes nos Tâches, ayant recours à un stimulus artificiel ou non, sont capables de produire un score cohérent et sont comparables entre elles. Nous pouvons alors dire d'une Tâche qu'elle est "Motivée" lorsqu'elle utilise un stimulus déclencheur artificiel.
			
			Nous avons désormais à notre disposition deux mécanismes influant sur le score d'une Tâche : nous pouvons jouer sur sa motivation source et modifier son seuil. Afin de garder une approche cohérente, nous proposons de modifier le seuil pour représenter l'état interne de l'agent (caractéristiques physiques, physiologiques, etc.). De la même manière, modifier la motivation source peut permettre de représenter un changement de perception ou une décision de la part d'un agent. Nous n'avons pas utilisé cette dernière option, la motivation source est conservée constante dans la totalité de nos implémentations. Nous ne jouons que sur la motivation guide de nos agents.
			
			Le score d'une Tâche Motivée ne représente alors plus sa priorité. Afin de résoudre ce non respect des hypothèses fondatrices des modèles à seuils, nous proposons un mécanisme d'interruption basé sur la motivation guide.
			
		\subsubsection{Motivation comme Mécanisme d'Interruption}
		
			La réévaluation systématique nous autorise à ne pas avoir de mécanisme d'interruption : dans le cas général, la fluctuation des stimulus internes et externes suffit à l'agent pour effectuer la bonne Tâche. En revanche, la question se pose lorsqu'une Tâche possède un stimulus déclencheur artificiel. Pour décider quand arrêter une telle Tâche, nous avons besoin de ce mécanisme d'interruption. Dans ce cas, nos agents vont essayer d'estimer leur apport à la communauté dans leur Tâche actuelle. Ainsi, lorsqu'un agent verra qu'il n'arrive pas à réaliser sa Tâche, il sera dans l'état de malaise décrit par le \textit{Flow} et cherchera de plus en plus à changer de Tâche. Nous cherchons alors à mesurer l'efficacité de chaque agent dans sa tâche, afin de pouvoir détecter lorsqu'il arrive à la réaliser, mais surtout lorsqu'il n'y arrive pas. Il suffit alors de déterminer dans le contenu de la Tâche quelles sont les Actions effectuées représentatives de son échec. Par exemple, un robot ayant pour tâche de récolter des ressources réalisera une séquence de déplacement aléatoire lorsqu'aucune ressource n'est en vue. Répéter en boucle ce déplacement aléatoire, cette Action, est un signe que ce robot n'arrive pas à correctement réaliser sa Tâche, il devrait donc essayer de faire autre chose.
			
			Nous proposons d'ajouter aux Actions définies plus tôt le fait de pouvoir être "Démotivantes" (notées \textbf{M-} dans nos schémas). Lors de l'exécution d'une Action Démotivante, un agent va baisser sa motivation interne (représentante de la Motivation Guide de l'agent) d'une valeur définie. Le but est d'augmenter les chances qu'il abandonne cette tâche au profit d'une autre, lors du processus de sélection. Rien d'aléatoire, mais plus une Tâche a un score élevé, moins il y a de chances qu'une autre Tâche soit sélectionnée à sa place. En effet, lors du calcul du score d'une Tâche, celui d'une Tâche Motivée est remplacé par la motivation interne de l'agent, seulement si c'est cette Tâche que l'agent exécutait au pas de temps précédent. Ainsi, une Tâche Motivée est sélectionnée grâce à son score provenant du stimulus artificiel, mais est interrompue par une valeur de motivation interne plus basse, qui aura tendance à favoriser d'autres Tâches. Lorsqu'une nouvelle Tâche Motivée est sélectionnée (et qu'elle n'était pas la Tâche sélectionnée au pas de temps précédent), la motivation interne de l'agent est remontée à sa valeur maximale.
			
			L'Action de déplacement aléatoire du robot que nous venons de citer est un bon exemple d'Action Démotivante. Une Action Démotivante doit forcément être dans une Tâche Motivée : si la motivation interne ne joue aucun rôle dans la sélection de la tâche, il n'y a aucun intérêt à la diminuer. Nous avons aussi fait le choix de ne pas intégrer de Tâches Motivantes dans le modèle, qui augmenterait la motivation interne de l'agent. Nous ne remontons la motivation interne qu'aux changements de Tâches. La Section \ref{sectionPerspectivesSMA} aborde notamment nos discussions et perspectives sur ces points.
			
			
	\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Pictures/Figures/ModelNow.png}
	\caption[Schéma de notre modèle de prise de décision]{Notre modèle de prise de décision, étendant les modèles à seuils grâce à l'utilisation de deux types de motivations internes, les motivations "Source" et "Guide".}
	\label{ModelNowPres}
	\end{figure}
			
			
			\paragraph{}			
			Dans la littérature nous trouvons les travaux de W. Agassounon et al. \cite{agassounon_scalable_2001}, qui sont assez proches de ce que nous proposons ici. Ils proposent de mesurer le temps depuis lequel un agent n'a pas réussi à être productif. Il mesure l'efficacité de chaque robot dans la tâche de collecte de ressource en regardant depuis combien de temps chaque agent n'a pas attrapé ou déposé une ressource. Lorsque ce temps dépasse un seuil, l'agent arrête alors ce qu'il fait et déclenche sa tâche de repos. Lorsque le temps de repos passe à son tour au-delà d'un certain seuil, l'agent reprend alors sa tâche de collecte de ressource. Ces transitions sont fixes et fonctionnent en couple, sa tâche de repos n'est sélectionnée que lorsque des agents ne parviennent pas à correctement réaliser leur tâche de collecte. Ainsi, notre modèle fonctionne sur le même principe mais étend la sélection et l'interruption aux modèles à seuils, permettant à de nombreuses tâches hétérogènes de cohabiter sans liens explicites.
			
			La Figure \ref{ModelNowPres} présente un schéma décrivant notre modèle de prise de décision. Nous y retrouvons les deux concepts de motivations que nous avons évoqués, les motivations Source et Guide. La motivation source permet de remplacer le stimulus déclencheur pour des Tâches qui n'en auraient pas (les Tâches Motivées), leur permettant de tout de même utiliser un modèle à seuil. Dans le cas d'une Tâche Motivée, le Score donné par le système à seuil, et donc le stimulus artificiel, est remplacé par la motivation Guide, et sert de mécanisme d'interruption. La motivation Guide permet à l'agent de se faire une idée de l'utilité de sa Tâche actuelle, afin de pouvoir l'interrompre si nécessaire.
			
	
	\section{Définir un Agent}	
	
		\paragraph{}
		À l'aide de ces définitions nous pouvons désormais décrire nos agents. Un agent est situé dans l'environnement, possède une série de senseurs internes et externes (comme la faim et l'odorat), et contient aussi une liste de Tâches qu'il sera peut-être amené à réaliser au cours de sa vie. Lors d'une sélection de tâche, l'agent pourra confronter ses perceptions courantes aux différents seuils et conditions de l'algorithme de sélection de tâche, donnés par son état interne ainsi que son environnement direct, afin de savoir quelle Action effectuer. La Figure \ref{agentExec} reprend ces notions et décrit le comportement et la prise de décision d'un agent à chaque pas de temps : lorsqu'un agent a terminé une Action il sélectionne une nouvelle Tâche en fonction de son état interne, de sa motivation interne et de ses perceptions. Il récupère et exécute ensuite l'Action à réaliser de sa nouvelle Tâche, grâce à l'architecture de subsomption hiérarchique de cette dernière.
		
		Parmi les perceptions internes de l'agent, nous trouvons une variable de motivation interne, servant aussi à la sélection de tâche. Associer une valeur de motivation à chaque Tâche aurait aussi été possible et nous pouvons en trouver des équivalences dans d'autres travaux cités précédemment, nous avons donc décidé de tenter l'expérience avec une valeur transversale à toutes les tâches, liée à l'agent lui-même, permettant de limiter le nombre de paramètres. La Section \ref{sectionPerspectivesSMA} aborde nos discussions sur ce point.
	
	\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Pictures/Figures/ModelisationExecution.png}
	\caption[Sélection et exécution des tâches par chaque Agent, à chaque pas de temps.]{Sélection et exécution des tâches par chaque Agent, à chaque pas de temps. Si l'Action en cours est terminée, l'agent va sélectionner une nouvelle Tâche, en extraire l'Action à réaliser puis l'exécuter.}
	\label{agentExec}
	\end{figure}	
		
		\paragraph{}
		Un agent peut aussi présenter des variations individuelles, un léger \textit{offset} que nous pouvons utiliser pour ajuster légèrement les seuils de ses différentes Tâches, en plus des conditions internes et externes, afin de créer une population d'agents plus ou moins hétérogène. Via ses Tâches, un agent possède des seuils variables qui lui sont propres : deux agents avec les mêmes Tâches présentent le plus souvent des seuils différents.
		
	\section{Exemple d'Application à la Robotique en Essaim}
	Pour illustrer notre modèle, nous en détaillons dans cette section un exemple d'application à la robotique en essaim.
			Les systèmes multi-agents sont souvent utilisés dans la mise en place d'essaims de robots, ce qu'on appelle le domaine des "Swarm Robotics". Ces essaims consistent en une grande quantité (dizaines, voire centaines) de robots simples, amenés à exécuter des tâches complexes en collectivité.
			
			L'image de la capacité de fourragement des fourmis est souvent utilisée comme cas d'application, nous avons donc construit notre exemple dans ce contexte. Un ensemble de robots va devoir ramener des ressources à leur base commune. Les ressources sont éparpillées dans l'environnement et doivent être traitées par un robot avant d'être déplacées jusqu'à la base. En plus de cette activité de collecte, les robots vont devoir assurer une surveillance de la base, en patrouillant autour. Ces robots possèdent une mémoire limitée : ils connaissent leur position ainsi que celle de la base, et peuvent se rappeler de la position de gisements de minerai qu'ils ont directement observés. Ils ne peuvent pas communiquer entre eux. Ils possèdent en revanche des capteurs leurs permettant de se voir entre eux.
			
		De plus, nous ajoutons la notion d'outil : un robot devra posséder le bon outil pour exécuter une Tâche, par exemple une pioche pour collecter des ressources, et des jumelles pour patrouiller. Ces outils représentent alors l'état physiologique du robot, visible sur la Figure \ref{ModelNowPres}. Les ressources brutes devront être traitées sur place avant de pouvoir être collectées puis amenées à la base. L'outil porté par un agent est visible des autres agents l'observant.

		\paragraph{}
		Nous pouvons dès lors commencer à construire nos Tâches :
		\begin{itemize}
			\item \textbf{Patrouiller}, Tâche Motivée décrite Figure \ref{swarmPatrol} : le robot effectue des cercles larges autour de la base, observant les alentours. Il se démotive lorsqu'il croise un autre robot (noté M- sur la Figure). Ceci permet aux robots d'éviter de patrouiller si un grand nombre de robots le fait déjà. Le seuil est élevé, sauf si l'agent est équipé des jumelles.
			\item \textbf{Collecter}, Tâche Motivée décrite Figure \ref{swarmForaging} : le robot parcourt l'environnement aléatoirement à la recherche d'un gisement. Une fois trouvé, il traite le gisement pour collecter des ressources, puis les ramène à la base. Il se démotive légèrement à chaque pas de temps où il exécute un déplacement aléatoire. Le seuil est élevé, sauf si l'agent est équipé d'une pioche.
			\item \textbf{Recharger} : le robot se connecte à la base pour recharger ses batteries.
			\item \textbf{Mémoriser} : lorsque le robot voit un gisement qu'il ne connait pas, il l'ajoute à sa mémoire. À l'inverse, lorsqu'il ne voit pas de gisement (ou qu'il voit un gisement épuisé) là où il en avait retenu un, il l'oublie. Ainsi, un robot qui patrouille peut découvrir de nouveaux gisements, tout autant qu'un robot qui en cherche activement un. Chaque robot pourra ensuite, dans sa Tâche de collecte, utiliser cette mémoire pour se rendre directement au gisement.
		\end{itemize}
	
	\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Pictures/Figures/SwarmPatrol.png}
	\caption{Robotique en essaim : modélisation de la Tâche de patrouille.}
	\label{swarmPatrol}
	\end{figure}
	
	\begin{figure}	
	\centering
	\includegraphics[width=\textwidth]{Pictures/Figures/SwarmForaging.png}
	\caption{Robotique en essaim : modélisation de la Tâche de collecte de ressources.}
	\label{swarmForaging}
	\end{figure}
		
		Pour \textit{Patrouiller} et \textit{Collecter}, si l'agent active la Tâche sans posséder le bon outil, celle-ci commencera par le faire retourner à la base pour s'équiper avec le bon. Cet outil va alors changer les seuils de ces deux Tâches, priorisant celle dont l'outil est le bon. Ainsi, un agent ne changera d'outil que lorsque cela sera nécessaire, les seuils ajoutent ici une notion de coût du changement d'outil. Ensuite, si nous le souhaitons, en ajustant les valeurs des seuils bas (prioritaires) et haut (changement d'outil nécessaire) et/ou l'intensité de la "démotivation" des Actions Démotivantes, nous pouvons ajuster ce coût empiriquement. Les Figures \ref{swarmPatrol} et \ref{swarmForaging} présentent respectivement nos modélisations en subsomption hiérarchique des Tâches de patrouille et de collecte de ressources. 
		
		Ensuite, si \textit{Recharger} a un stimulus déclencheur évident, le niveau courant de batterie, ce n'est pas le cas pour \textit{Patrouiller} et \textit{Collecter}. Nous leur appliquons donc un stimulus artificiel. Nous pouvons ensuite modifier légèrement ces stimulus artificiels avec des perceptions de l'agent : nous pouvons réduire légèrement l'intensité du stimulus artificiel pour la Tâche \textit{Patrouiller} lorsqu'un robot avec des jumelles est dans le champ de vision. De même, nous pouvons réduire la stimulation de \textit{Collecter} lorsqu'un robot avec une pioche est en vue, et l'augmenter lorsqu'un gisement de minerai est en vue.
		
		Cet exemple d'implémentation du modèle de répartition des tâches dans le cadre d'un essaim de robots a fait l'objet d'un projet étudiant de Master 1.
			
			
				
	\section*{Synthèse}
		Dans ce chapitre nous avons décrit notre modélisation des Tâches en Activités et Actions, ainsi que la modification des modèles à seuils pour notre sélection de tâches. Cette sélection se fait via un système à seuils adapté afin de prendre en compte les Motivations Source (nous venant plutôt de l'éthologie) et Guide (ou interne, nous venant de la théorie du \textit{Flow}), afin d'inclure des tâches ne présentant pas de stimulus déclencheur défini. Une Tâche sans stimulus déclencheur se voit affectée une motivation source, sous la forme d'un stimulus artificiel, permettant de calculer le score de la Tâche de la même manière que les Tâches possédant un stimulus déclencheur. Un score est calculé pour chaque Tâche selon les perceptions et l'état interne de l'agent, et ce dernier sélectionne la Tâche qui possède le plus élevé. Lorsque la Tâche précédemment réalisée est une Tâche Motivée, son score est remplacé par la Motivation Guide, interne à l'agent et transversale à toutes ses Tâches. Une fois la Tâche sélectionnée, l'agent utilise notre architecture de Tâches en subsomption hiérarchique afin d'obtenir le comportement, l'Action, qu'il doit réaliser. Certaines Actions peuvent être Démotivantes : un agent qui exécute une Action Démotivante va réduire sa Motivation Guide, augmentant ainsi ses chances de changer de Tâche au prochain pas de temps. On parle alors de mécanisme d'interruption. Nous avons décrit un exemple possible d'application à un essaim de robots, et nous allons désormais pouvoir aborder l'implémentation de l'application principale de ces travaux, la colonie d'abeilles et son simulateur.
